{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitc30deacdfc314bf0ab55258624adc68b",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "https://github.com/datacamp/course-resources-ml-with-experts-budgets\n",
    "## Topic: \n",
    "School Budgets problem from DrivenData.org\n",
    "School budgets are huge, complex and non-standardized within the US\n",
    "School want to measure their performance\n",
    "## Goal: \n",
    "Build Machine Learning Algorithm that automate the labeling of spendings\n",
    "## Data:\n",
    "Line-Data with description:\n",
    "like \"Algebra books for 8th grade students\"\n",
    "Labels attached like: \"Math\", \"Middle School\", \"Textbooks\"\n",
    "## Type of the problem:\n",
    "Supervised Learning Problem -> Using correct labeled data to predict the label of an unlabeled sample\n",
    "predict the label -> Classification problem\n",
    "predict the probability of each target variables possible value (logreg?)\n",
    "## Specials:\n",
    "Over 100 target variables\n",
    "9 columns with several possible Labels\n",
    "predicting variable-value-probabilities via dummy variables\n",
    "## Exploring the data:\n",
    "Loading dataset via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unnamed: 0                 Function          Use          Sharing  \\\n0      134338     Teacher Compensation  Instruction  School Reported   \n1      206341                 NO_LABEL     NO_LABEL         NO_LABEL   \n2      326408     Teacher Compensation  Instruction  School Reported   \n3      364634  Substitute Compensation  Instruction  School Reported   \n4       47683  Substitute Compensation  Instruction  School Reported   \n\n  Reporting Student_Type Position_Type               Object_Type     Pre_K  \\\n0    School     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n1  NO_LABEL     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n2    School  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n3    School  Unspecified    Substitute                  Benefits  NO_LABEL   \n4    School  Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n\n    Operating_Status  ... Sub_Object_Description Location_Description  FTE  \\\n0  PreK-12 Operating  ...                    NaN                  NaN  1.0   \n1      Non-Operating  ...                    NaN                  NaN  NaN   \n2  PreK-12 Operating  ...                    NaN                  NaN  1.0   \n3  PreK-12 Operating  ...                    NaN                  NaN  NaN   \n4  PreK-12 Operating  ...                    NaN                  NaN  NaN   \n\n      Function_Description Facility_or_Department              Position_Extra  \\\n0                      NaN                    NaN               KINDERGARTEN    \n1                 RGN  GOB                    NaN                UNDESIGNATED   \n2                      NaN                    NaN                     TEACHER   \n3  UNALLOC BUDGETS/SCHOOLS                    NaN  PROFESSIONAL-INSTRUCTIONAL   \n4              NON-PROJECT                    NaN  PROFESSIONAL-INSTRUCTIONAL   \n\n       Total             Program_Description        Fund_Description  \\\n0  50471.810                    KINDERGARTEN            General Fund   \n1   3477.860   BUILDING IMPROVEMENT SERVICES                     NaN   \n2  62237.130           Instruction - Regular  General Purpose School   \n3     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH                     NaN   \n4     54.166   GENERAL HIGH SCHOOL EDUCATION                     NaN   \n\n                          Text_1  \n0                            NaN  \n1  BUILDING IMPROVEMENT SERVICES  \n2                            NaN  \n3            REGULAR INSTRUCTION  \n4            REGULAR INSTRUCTION  \n\n[5 rows x 26 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 400277 entries, 0 to 400276\nData columns (total 26 columns):\nUnnamed: 0                400277 non-null int64\nFunction                  400277 non-null object\nUse                       400277 non-null object\nSharing                   400277 non-null object\nReporting                 400277 non-null object\nStudent_Type              400277 non-null object\nPosition_Type             400277 non-null object\nObject_Type               400277 non-null object\nPre_K                     400277 non-null object\nOperating_Status          400277 non-null object\nObject_Description        375493 non-null object\nText_2                    88217 non-null object\nSubFund_Description       306855 non-null object\nJob_Title_Description     292743 non-null object\nText_3                    109152 non-null object\nText_4                    53746 non-null object\nSub_Object_Description    91603 non-null object\nLocation_Description      162054 non-null object\nFTE                       126071 non-null float64\nFunction_Description      342195 non-null object\nFacility_or_Department    53886 non-null object\nPosition_Extra            264764 non-null object\nTotal                     395722 non-null float64\nProgram_Description       304660 non-null object\nFund_Description          202877 non-null object\nText_1                    292285 non-null object\ndtypes: float64(2), int64(1), object(23)\nmemory usage: 79.4+ MB\nNone\n          Unnamed: 0            FTE         Total\ncount  400277.000000  126071.000000  3.957220e+05\nmean   225186.018537       0.426794  1.310586e+04\nstd    130025.142718       0.573576  3.682254e+05\nmin         2.000000      -0.087551 -8.746631e+07\n25%    112601.000000       0.000792  7.379770e+01\n50%    225243.000000       0.130927  4.612300e+02\n75%    337722.000000       1.000000  3.652662e+03\nmax    450340.000000      46.800000  1.297000e+08\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.frame.DataFrame'>\n"
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "\n",
    "#load a sample set of the data  !!!!!!!!!!!!! -- Change to the real dataset\n",
    "sample_df = pd.read_csv(\"TrainingData.csv\")\n",
    "\n",
    "#explore basics via head (data example), info(data structure) and describe(summary statistics)\n",
    "print(sample_df.head())\n",
    "print( sample_df.info())\n",
    "print(sample_df.describe())\n",
    "NUMERIC_COLUMNS = list(sample_df.loc[:,sample_df.dtypes != \"object\"].columns)\n",
    "LABELS = ['Function',\n",
    " 'Use',\n",
    " 'Sharing',\n",
    " 'Reporting',\n",
    " 'Student_Type',\n",
    " 'Position_Type',\n",
    " 'Object_Type',\n",
    " 'Pre_K',\n",
    " 'Operating_Status']\n",
    "df = sample_df\n",
    "\n",
    "print(type(df[\"FTE\"]))\n",
    "print(type(df[[\"FTE\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information obtained:\n",
    "* Strings in values in several columns\n",
    "* NaNs found in several columns \n",
    "\n",
    "### Encountered problems\n",
    "* ML works with numbers not with strings \n",
    "* Strings are computationally expensive\n",
    "    * Category - Datatype from pandas could solve the issue by storing this information numerically\n",
    "\n",
    "### What to do \n",
    "create lambda function to change each object type column into categorical column\n",
    "\n",
    "    categorize = lambda x = x.astype(\"category\")\n",
    "\n",
    "Apply this function to desired column using .apply()-method\n",
    "\n",
    "    sample_df.label = sample_df[[\"label\"]].apply(categorize, axis=0) ((use a list of column labels))\n",
    "\n",
    "Count the numbers of unique categorical values per Category via .apply(pd.Series.nunique)\n",
    "\n",
    "    num_unique_values = sample_df[[\"label\"]].apply(pd.Series.nunique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of model sucess\n",
    "### Logloss function\n",
    "The Logloss-function is used in this competition to evaluate the model performace\n",
    "\n",
    "see:  https://www.drivendata.org/competitions/46/box-plots-for-education-reboot/submissions/\n",
    "\n",
    "-> being less sure is better than confident and wrong\n",
    "\n",
    "Loglossfunction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6931471805599453"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    predicted = np.clip(predicted, eps, 1-eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "                +(1-actual)\n",
    "                *np.log(1-predicted))\n",
    "    return loss\n",
    "compute_log_loss(0.5, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with a simple model\n",
    "gives a sense of how complex and difficult the problem might be  \n",
    "wanting to come as fast as possible from raw data to prediction  \n",
    "Using Multi-class logistic regression  \n",
    "Format predictions and save to csv  \n",
    "Submit\n",
    "\n",
    "### Splitting the dataset\n",
    "Normal train-test-split does not work here, because of many different labels\n",
    "\n",
    "### Solution:  \n",
    "\n",
    "    StratifiedShuffleSplit\n",
    "* Con: Only works with single target variable\n",
    "* We have many target variables\n",
    "* multilabel_train_test_split()  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from warnings import warn\n",
    "\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    predicted = np.clip(predicted, eps, 1-eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "                +(1-actual)\n",
    "                *np.log(1-predicted))\n",
    "    return loss\n",
    "compute_log_loss(0.5, 0)\n",
    "\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).all():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n",
    "\n",
    "BOX_PLOTS_COLUMN_INDICES = [list(range(37)),\n",
    "                            list(range(37, 48)),\n",
    "                            list(range(48, 51)),\n",
    "                            list(range(51, 76)),\n",
    "                            list(range(76, 79)),\n",
    "                            list(range(79, 82)),\n",
    "                            list(range(82, 87)),\n",
    "                            list(range(87, 96)),\n",
    "                            list(range(96, 104))]\n",
    "\n",
    "\n",
    "def multi_multi_log_loss(predicted,\n",
    "                          actual,\n",
    "                          class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
    "                          eps=1e-15):\n",
    "    \"\"\" Multi class version of Logarithmic Loss metric as implemented on\n",
    "        DrivenData.org\n",
    "    \"\"\"\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "\n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
    "\n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions so\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "\n",
    "    return np.average(class_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to:\n",
    "\n",
    "Minimal preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Unnamed: 0', 'FTE', 'Total']\n['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'multilabel_train_test_split' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c764758d4111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_to_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNUMERIC_COLUMNS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlabels_to_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, \n\u001b[0m\u001b[0;32m      7\u001b[0m                                                                \u001b[0mlabels_to_use\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                                \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multilabel_train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "print(NUMERIC_COLUMNS)  #list of all numeric columns \n",
    "print(LABELS)#List of target label columns\n",
    "\n",
    "data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, \n",
    "                                                               labels_to_use, \n",
    "                                                               size =0.2, \n",
    "                                                               seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.linar_model'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-349d12106d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinar_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.linar_model'"
     ]
    }
   ],
   "source": [
    "from sklearn.linar_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression()) \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "    OneVsRestClassifier\n",
    "   \n",
    "treats each column of y independently  \n",
    "Fits a separate classifier for each of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating competing in the Competition\n",
    "## Loading Test_set (Holdout data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not in index\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dfa87d315be1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mholdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TestData.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mholdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mholdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNUMERIC_COLUMNS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2999\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3001\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not in index\""
     ]
    }
   ],
   "source": [
    "holdout = pd.read_csv(\"TestData.csv\", index_col=0)\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the use of Logloss, .predict() would be much worse (only 0s and 1s)  \n",
    "-> Using **.predict_proba()** solves the problem giving probabilities rather than predictions  \n",
    "  \n",
    "  \n",
    "## Formatting and submission of predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS], prefix_sep=\"__\").columns, index=holdout.index, data=predictions)\n",
    "prediction.df.to_csv(\"precitions.csv\")\n",
    "score = score_submission(pred_path=\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and obtain score from the Leaderboard\n",
    "\n",
    "# Introduction to NLP\n",
    "Data: Text, documents, speech\n",
    "\n",
    "## First step: Tokenization  \n",
    "* Splitting a string into segments\n",
    "* Store results as lists  \n",
    "Example:  \n",
    "\"Natural Language Processing\"  \n",
    "  \n",
    "\"Natural\", \"Language\",\"Processing\"  \n",
    "\n",
    "Tokenize on:  \n",
    "* whitespace\n",
    "* punctuation\n",
    "\n",
    "## Using \"bag of words\"\n",
    "* Count the number of times a particular token appears\n",
    "* Bag of words:  \n",
    "  * Count the number of times a word was pulled out of the bag  \n",
    "* This approach discards the information about word order\n",
    "->\"Red, not blue\" == \"blue, not red\"  \n",
    "  \n",
    "## Using n-gram\n",
    "1-gram, 2-gram,...,n-gram\n",
    "\n",
    "Not a single word is treated as an occurance to count but every ordered 2 word pair = \"2-gram\"\n",
    "  \n",
    "## Representing words numerically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bag-of-Word representation\n",
    "Using: sklearns CountVectorizor\n",
    "\n",
    "does 3 things:\n",
    "    Tokenize all strings\n",
    "    Builds a vocabulary\n",
    "    Counts the tokens apprearance\n",
    "How?'''\n",
    "\n",
    "#import packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#define regular_expression that does the split on whitespaces\n",
    "TOKEN_BASIC = \"\\\\\\\\S+(=\\\\\\\\s+)\"\n",
    "\n",
    "#replace NaNs with empty strings\n",
    "df.Program_Description.fillna(\"\", inplace = True)\n",
    "\n",
    "#instantiate CountVectorizer\n",
    "vec_basic = CountVectorizer(token_pattern=TOKEN_BASIC)\n",
    "\n",
    "#fit the vectorizer\n",
    "vec_basic.fit(df.Program_description)\n",
    "\n",
    "#Extract found words by using get_feature_names()-method on vectorizer\n",
    "features = vec_basic.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline, features & test processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "= Repeatable way to go from raw data to trained model ->see unsupervised-learning notes\n",
    "  \n",
    "Pipelines can also have sub-pipelines as steps\n",
    "  \n",
    "## How to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"None of [Index(['numeric'], dtype='object')] are in the [columns]\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-726adc6d033b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#split data !![[for getting a dataframe instead of a Series]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"numeric\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2999\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3001\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\envs\\py3-TF2.0\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 raise KeyError(\n\u001b[0;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1177\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m                     )\n\u001b[0;32m   1179\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['numeric'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#import packages:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#initiate pipeline\n",
    "pl = Pipeline([(\"clf\", OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "#split data !![[for getting a dataframe instead of a Series]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"numeric\"]], pd.get_dummies(df[\"label\"]), random_state = 2)\n",
    "\n",
    "#fit\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "#score on test\n",
    "pl.score(X_test, y_test)\n",
    "\n",
    "#Throwing with NaNs will cause a break_down\n",
    "#Therefore: create Imputer (convert NaNs)\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pl = Pipeline([(\"imp\", Imputer),\n",
    "               (\"clf\", OneVsRestClassifier(LogisticRegression())\n",
    "                )])\n",
    "\n",
    "#fit and score with nans\n",
    "\n",
    "\n",
    "## Implementing text features into the pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], pd.get_dummies(df[\"label\"]), random_state=2)\n",
    "\n",
    "pl = Pipeline([(\"vec\" , CountVectorizer()),\n",
    "                (\"clf\" , OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "#fit and score again\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing multiple dtypes\n",
    "* Using all variables from different types\n",
    "* Problem:\n",
    "    * Pipelines cannot follow each other \n",
    "    * e.g., CountVec can't be input for Imputer\n",
    "* Solution: FunctionTransformer() & FeatureUnion()  \n",
    "  \n",
    "\n",
    "### Functiontransformer()\n",
    "* Turn Python function into object, understandable by scikit-learn Pipelines  \n",
    "* Need to write two functions for pipeline preprocessing  \n",
    "    1 Takes hole dataframe, returns numeric columns  \n",
    "2 Takes hole dataframe, returns text columns\n",
    "\n",
    "## How to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#split complete set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"numeric\", \"with_missing\", \"text\"]], pd.get_dummies(df[\"label\"]), random_state = 1)\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x[\"text\"], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[[\"numeric\", \"with_missing\"]], validate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnion\n",
    "combines the two outcomes into one dataframe\n",
    "\n",
    "## HowTo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([(\"numeric\", numeric_pipeline),\n",
    "                     (\"text\", text_pipeline)])                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#split complete set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"numeric\", \"with_missing\", \"text\"]], pd.get_dummies(df[\"label\"]), random_state = 1)\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x[\"text\"], validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[[\"numeric\", \"with_missing\"]], validate = False)\n",
    "#to obtain results use fit_transform(x)\n",
    "\n",
    "\n",
    "#create numeric sub-pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "                            (\"selector\", get_numeric_data),\n",
    "                            (\"imputer\", Imputer())\n",
    "                            ])\n",
    "\n",
    "#create text sub-pipeline\n",
    "text_pipeline = Pipeline([\n",
    "                        (\"selector\", get_text_data),\n",
    "                        (\"vectorizer\"), CountVectorizer())\n",
    "                        ])\n",
    "#create final pipeline with featureunion and two subs + model\n",
    "pl = Pipeline([\n",
    "                (\"union\", FeatureUnion([\n",
    "                                        (\"numeric\", numeric_pipeline)\n",
    "                                        (\"text\", text_pipeline)\n",
    "                                        ])),\n",
    "                (\"clf\" , OneVsRestClassifier(LogisticRegression()))\n",
    "                ])\n",
    "\n",
    "#fit the data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "#get the score\n",
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Object_Description', 'Text_2', 'SubFund_Description', 'Job_Title_Description', 'Text_3', 'Text_4', 'Sub_Object_Description', 'Location_Description', 'FTE', 'Function_Description', 'Facility_or_Department', 'Position_Extra', 'Total', 'Program_Description', 'Fund_Description', 'Text_1']\n['FTE', 'Total']\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.4583388290318766"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "#Using the pipeline with the main dataset\n",
    "\n",
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "log_loss_scorer = make_scorer(multi_multi_log_loss)\n",
    "\n",
    "LABELS = ['Function',\n",
    " 'Use',\n",
    " 'Sharing',\n",
    " 'Reporting',\n",
    " 'Student_Type',\n",
    " 'Position_Type',\n",
    " 'Object_Type',\n",
    " 'Pre_K',\n",
    " 'Operating_Status']\n",
    "#NON_LABELS = list(df.loc[:,~df.columns.isin(LABELS)].columns)\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS] #much better approach\n",
    "NUMERIC_COLUMNS = list(df.loc[:,df.dtypes != \"object\"].columns)\n",
    "print(NON_LABELS)\n",
    "print(NUMERIC_COLUMNS)\n",
    "\n",
    "df = pd.read_csv(\"TrainingData.csv\", index_col=0, nrows=20000)\n",
    "\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS], dummy_labels, 0.2, 1)\n",
    "\n",
    "\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" Takes the dataset as read in, drops the non-feature, non-text columns and\n",
    "        then combines all of the text columns into a single vector that has all of\n",
    "        the text for a row.\n",
    "        \n",
    "        :param data_frame: The data as read in with read_csv (no preprocessing necessary)\n",
    "        :param to_drop (optional): Removes the numeric and label columns by default.\n",
    "    \"\"\"\n",
    "    # drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # joins all of the text items in a row (axis=1)\n",
    "    # with a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "pl = Pipeline([(\"union\",FeatureUnion([\n",
    "                        (\"numeric_features\", Pipeline([\n",
    "                                            (\"selector\", get_numeric_data),\n",
    "                                            (\"imputer\", SimpleImputer()),\n",
    "                                            (\"scaler\", MaxAbsScaler())\n",
    "                                                        ])),\n",
    "                        (\"text_features\", Pipeline([\n",
    "                                        (\"selector\", get_text_data),\n",
    "                                        (\"vectorizer\", CountVectorizer(ngram_range=(1,2))\n",
    "                                        )\n",
    "                                                    ]))\n",
    "                                        ])),\n",
    "                                \n",
    "                (\"clf\", OneVsRestClassifier(LogisticRegression()))\n",
    "              ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "log_loss_scorer(pl, X_test, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks\n",
    "## Text Processing\n",
    "* NLP Tricks  \n",
    "\n",
    "* Tokenized on punctutation\n",
    "to avoid hyphens, underscores, etc  \n",
    "  \n",
    "* Include a unigrams and bigrams in the model via CountVectorizer\n",
    "  \n",
    "## Stats trick\n",
    "* Interaction terms -> describes mathematically when tokens appear together by adding a beta3*(x1 * x2) -1*0 = 0, 1*1 = 1, etc.-  \n",
    "  \n",
    "* use via **from sklearn.preprocessing import PolynominalFeatures**  \n",
    "  \n",
    "* interaction = PolynominalFeatures(degree = 2, interaction_only = True, include_bias = False) \n",
    "  * use SparseInteractions instead\n",
    "  \n",
    "* Fit and transform to get the new slide\n",
    "\n",
    "## The winning model  \n",
    "* Adding new features will cause enourmous increase in array size -> computaional power  \n",
    "  \n",
    "* Hashing to be more memory-efficient  \n",
    "  \n",
    "* -> Dimensionality reduction  \n",
    "  \n",
    "* How to: use HashingVectorizer instead with (norm=None, non_negative = True, token_pattern = TOKEN_ALPHANUMERIC, ngram_range = (1, 2))  \n",
    "  \n",
    "## Other Tricks\n",
    "\n",
    "* NLP:Stemming, Stop_word removal\n",
    "* Model: RandomForest, kNN, Naive Bayes\n",
    "* Numeric Processing :Imputation strategies\n",
    "* Optimization: GridSearchCV\n",
    "* Experiement with other techniques\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}